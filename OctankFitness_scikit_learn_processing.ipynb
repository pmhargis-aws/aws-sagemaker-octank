{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Awesome Builder Notebook\n",
    "\n",
    "We will use Amazon SageMaker Processing jobs to leverage a simplified, managed experience to run data pre- or post-processing and model evaluation workloads on the Amazon SageMaker platform.\n",
    "\n",
    "A processing job downloads input from Amazon Simple Storage Service (Amazon S3), then uploads outputs to Amazon S3 during or after the processing job.\n",
    "\n",
    "This notebook is used to run a processing job using a scikit-learn script that cleans, pre-processes, performs feature engineering, and splits the input data into train and test sets.\n",
    "\n",
    "The dataset loaded here is from PAMAP2 Physical Activity Dataset from UCI database:\n",
    "https://archive.ics.uci.edu/ml/datasets/PAMAP2+Physical+Activity+Monitoring\n",
    "\n",
    "We will select features from this dataset, clean the data, and turn the data into features that the training algorithm can use to train a multi-class classification model, and split the data into train and test sets. \n",
    "\n",
    "After training a logistic regression model, you evaluate the model against a hold-out test dataset, and save the classification evaluation metrics, including precision, recall, and F1 score for each label, and accuracy and ROC AUC for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing and feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the scikit-learn preprocessing script as a processing job, create a `SKLearnProcessor`, which lets you run scripts inside of processing jobs using the scikit-learn image provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-1\n",
      "Role: arn:aws:iam::572539092864:role/service-role/AmazonSageMaker-ExecutionRole-20200407T174741\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_type='ml.m5.xlarge',\n",
    "                                     instance_count=1)\n",
    "\n",
    "print('Region: {}'.format(region))\n",
    "print('Role: {}'.format(role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before writing the script we will use for data cleaning, pre-processing, and feature engineering, we inspect the first 10 rows (observations) of the dataset. \n",
    "\n",
    "The initial label for the observation is the `activity_id` category. The primary features from the dataset you select are `heart_rate`, `imu_wrist_temp` (temperature in Celsius), and 3 sets of Inertial Measurement Units (IMU) data. IMU data is collected from 3 separate sensors located on wearable devices, like a Smart Watch, plus a heart rate monitor. \n",
    "\n",
    "The dataset can be used for activity recognition and intensity estimation, while developing and applying algorithms of data processing, segmentation, feature extraction and classification.\n",
    "\n",
    "** Sensors **\n",
    "3 Colibri wireless inertial measurement units (IMU):\n",
    "- sampling frequency: 100Hz\n",
    "- position of the sensors:\n",
    "- 1 IMU over the wrist on the dominant arm\n",
    "- 1 IMU on the chest\n",
    "- 1 IMU on the dominant side's ankle\n",
    "HR-monitor:\n",
    "- sampling frequency: ~9Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 input URI: s3://octank-smartwatch-data/sagemaker-train/subject101.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "bucket_name = 'octank-smartwatch-data'\n",
    "s3prefix = 'sagemaker-train'\n",
    "\n",
    "input_data_uri = 's3://{}/{}/subject101.csv'.format(bucket_name, s3prefix)\n",
    "\n",
    "print('S3 input URI: {}'.format(input_data_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['timestamp', 'activity_id', 'heart_rate', 'imu_wrist_temp', \n",
    "  'imu_wrist_accel16_x', 'imu_wrist_accel16_y', 'imu_wrist_accel16_z', \n",
    "  'imu_wrist_accel6_x', 'imu_wrist_accel6_y', 'imu_wrist_accel6_z', \n",
    "  'imu_wrist_gyro_x', 'imu_wrist_gyro_y', 'imu_wrist_gyro_z', \n",
    "  'imu_wrist_magnet_x', 'imu_wrist_magnet_y', 'imu_wrist_magnet_z', \n",
    "  'imu_wrist_orient1', 'imu_wrist_orient2', 'imu_wrist_orient3', 'imu_wrist_orient4', \n",
    "  'imu_chest_temp', 'imu_chest_accel16_x', 'imu_chest_accel16_y', 'imu_chest_accel16_z', \n",
    "  'imu_chest_accel6_x', 'imu_chest_accel6_y', 'imu_chest_accel6_z', \n",
    "  'imu_chest_gyro_x', 'imu_chest_gyro_y', 'imu_chest_gyro_z', \n",
    "  'imu_chest_magnet_x', 'imu_chest_magnet_y', 'imu_chest_magnet_z', \n",
    "  'imu_chest_orient1', 'imu_chest_orient2', 'imu_chest_orient3', 'imu_chest_orient4', \n",
    "  'imu_ankle_temp', 'imu_ankle_accel16_x', 'imu_ankle_accel16_y', 'imu_ankle_accel16_z', \n",
    "  'imu_ankle_accel6_x', 'imu_ankle_accel6_y', 'imu_ankle_accel6_z', \n",
    "  'imu_ankle_gyro_x', 'imu_ankle_gyro_y', 'imu_ankle_gyro_z', \n",
    "  'imu_ankle_magnet_x', 'imu_ankle_magnet_y', 'imu_ankle_magnet_z', \n",
    "  'imu_ankle_orient1', 'imu_ankle_orient2', 'imu_ankle_orient3', 'imu_ankle_orient4']\n",
    "\n",
    "df = pd.read_csv(input_data_uri, header=0, names=column_names, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 376416 entries, 0 to 376415\n",
      "Data columns (total 54 columns):\n",
      " #   Column               Non-Null Count   Dtype  \n",
      "---  ------               --------------   -----  \n",
      " 0   timestamp            376416 non-null  float64\n",
      " 1   activity_id          376416 non-null  int64  \n",
      " 2   heart_rate           34388 non-null   float64\n",
      " 3   imu_wrist_temp       374962 non-null  float64\n",
      " 4   imu_wrist_accel16_x  374962 non-null  float64\n",
      " 5   imu_wrist_accel16_y  374962 non-null  float64\n",
      " 6   imu_wrist_accel16_z  374962 non-null  float64\n",
      " 7   imu_wrist_accel6_x   374962 non-null  float64\n",
      " 8   imu_wrist_accel6_y   374962 non-null  float64\n",
      " 9   imu_wrist_accel6_z   374962 non-null  float64\n",
      " 10  imu_wrist_gyro_x     374962 non-null  float64\n",
      " 11  imu_wrist_gyro_y     374962 non-null  float64\n",
      " 12  imu_wrist_gyro_z     374962 non-null  float64\n",
      " 13  imu_wrist_magnet_x   374962 non-null  float64\n",
      " 14  imu_wrist_magnet_y   374962 non-null  float64\n",
      " 15  imu_wrist_magnet_z   374962 non-null  float64\n",
      " 16  imu_wrist_orient1    374962 non-null  float64\n",
      " 17  imu_wrist_orient2    374962 non-null  float64\n",
      " 18  imu_wrist_orient3    374962 non-null  float64\n",
      " 19  imu_wrist_orient4    374962 non-null  float64\n",
      " 20  imu_chest_temp       375907 non-null  float64\n",
      " 21  imu_chest_accel16_x  375907 non-null  float64\n",
      " 22  imu_chest_accel16_y  375907 non-null  float64\n",
      " 23  imu_chest_accel16_z  375907 non-null  float64\n",
      " 24  imu_chest_accel6_x   375907 non-null  float64\n",
      " 25  imu_chest_accel6_y   375907 non-null  float64\n",
      " 26  imu_chest_accel6_z   375907 non-null  float64\n",
      " 27  imu_chest_gyro_x     375907 non-null  float64\n",
      " 28  imu_chest_gyro_y     375907 non-null  float64\n",
      " 29  imu_chest_gyro_z     375907 non-null  float64\n",
      " 30  imu_chest_magnet_x   375907 non-null  float64\n",
      " 31  imu_chest_magnet_y   375907 non-null  float64\n",
      " 32  imu_chest_magnet_z   375907 non-null  float64\n",
      " 33  imu_chest_orient1    375907 non-null  float64\n",
      " 34  imu_chest_orient2    375907 non-null  float64\n",
      " 35  imu_chest_orient3    375907 non-null  float64\n",
      " 36  imu_chest_orient4    375907 non-null  float64\n",
      " 37  imu_ankle_temp       375089 non-null  float64\n",
      " 38  imu_ankle_accel16_x  375089 non-null  float64\n",
      " 39  imu_ankle_accel16_y  375089 non-null  float64\n",
      " 40  imu_ankle_accel16_z  375089 non-null  float64\n",
      " 41  imu_ankle_accel6_x   375089 non-null  float64\n",
      " 42  imu_ankle_accel6_y   375089 non-null  float64\n",
      " 43  imu_ankle_accel6_z   375089 non-null  float64\n",
      " 44  imu_ankle_gyro_x     375089 non-null  float64\n",
      " 45  imu_ankle_gyro_y     375089 non-null  float64\n",
      " 46  imu_ankle_gyro_z     375089 non-null  float64\n",
      " 47  imu_ankle_magnet_x   375089 non-null  float64\n",
      " 48  imu_ankle_magnet_y   375089 non-null  float64\n",
      " 49  imu_ankle_magnet_z   375089 non-null  float64\n",
      " 50  imu_ankle_orient1    375089 non-null  float64\n",
      " 51  imu_ankle_orient2    375089 non-null  float64\n",
      " 52  imu_ankle_orient3    375089 non-null  float64\n",
      " 53  imu_ankle_orient4    375089 non-null  float64\n",
      "dtypes: float64(53), int64(1)\n",
      "memory usage: 155.1 MB\n"
     ]
    }
   ],
   "source": [
    "# inspect DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(376416, 54)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>activity_id</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>imu_wrist_temp</th>\n",
       "      <th>imu_wrist_accel16_x</th>\n",
       "      <th>imu_wrist_accel16_y</th>\n",
       "      <th>imu_wrist_accel16_z</th>\n",
       "      <th>imu_wrist_accel6_x</th>\n",
       "      <th>imu_wrist_accel6_y</th>\n",
       "      <th>imu_wrist_accel6_z</th>\n",
       "      <th>...</th>\n",
       "      <th>imu_ankle_gyro_x</th>\n",
       "      <th>imu_ankle_gyro_y</th>\n",
       "      <th>imu_ankle_gyro_z</th>\n",
       "      <th>imu_ankle_magnet_x</th>\n",
       "      <th>imu_ankle_magnet_y</th>\n",
       "      <th>imu_ankle_magnet_z</th>\n",
       "      <th>imu_ankle_orient1</th>\n",
       "      <th>imu_ankle_orient2</th>\n",
       "      <th>imu_ankle_orient3</th>\n",
       "      <th>imu_ankle_orient4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.39</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.18837</td>\n",
       "      <td>8.56560</td>\n",
       "      <td>3.66179</td>\n",
       "      <td>2.39494</td>\n",
       "      <td>8.55081</td>\n",
       "      <td>3.64207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006577</td>\n",
       "      <td>-0.004638</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>-59.8479</td>\n",
       "      <td>-38.8919</td>\n",
       "      <td>-58.5253</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.40</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.37357</td>\n",
       "      <td>8.60107</td>\n",
       "      <td>3.54898</td>\n",
       "      <td>2.30514</td>\n",
       "      <td>8.53644</td>\n",
       "      <td>3.73280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003014</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.022495</td>\n",
       "      <td>-60.7361</td>\n",
       "      <td>-39.4138</td>\n",
       "      <td>-58.3999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.41</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.07473</td>\n",
       "      <td>8.52853</td>\n",
       "      <td>3.66021</td>\n",
       "      <td>2.33528</td>\n",
       "      <td>8.53622</td>\n",
       "      <td>3.73277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>-0.020301</td>\n",
       "      <td>0.011275</td>\n",
       "      <td>-60.4091</td>\n",
       "      <td>-38.7635</td>\n",
       "      <td>-58.3956</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.42</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.22936</td>\n",
       "      <td>8.83122</td>\n",
       "      <td>3.70000</td>\n",
       "      <td>2.23055</td>\n",
       "      <td>8.59741</td>\n",
       "      <td>3.76295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012698</td>\n",
       "      <td>-0.014303</td>\n",
       "      <td>-0.002823</td>\n",
       "      <td>-61.5199</td>\n",
       "      <td>-39.3879</td>\n",
       "      <td>-58.2694</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.43</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.29959</td>\n",
       "      <td>8.82929</td>\n",
       "      <td>3.54710</td>\n",
       "      <td>2.26132</td>\n",
       "      <td>8.65762</td>\n",
       "      <td>3.77788</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006089</td>\n",
       "      <td>-0.016024</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>-60.2954</td>\n",
       "      <td>-38.8778</td>\n",
       "      <td>-58.3977</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.44</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.33738</td>\n",
       "      <td>8.82900</td>\n",
       "      <td>3.54767</td>\n",
       "      <td>2.27703</td>\n",
       "      <td>8.77828</td>\n",
       "      <td>3.73230</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031973</td>\n",
       "      <td>-0.053934</td>\n",
       "      <td>0.015594</td>\n",
       "      <td>-60.6307</td>\n",
       "      <td>-38.8676</td>\n",
       "      <td>-58.2711</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.45</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.37142</td>\n",
       "      <td>9.05500</td>\n",
       "      <td>3.39347</td>\n",
       "      <td>2.39786</td>\n",
       "      <td>8.89814</td>\n",
       "      <td>3.64131</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019643</td>\n",
       "      <td>-0.039937</td>\n",
       "      <td>-0.000785</td>\n",
       "      <td>-60.5171</td>\n",
       "      <td>-38.9819</td>\n",
       "      <td>-58.2733</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.46</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.33951</td>\n",
       "      <td>9.13251</td>\n",
       "      <td>3.54668</td>\n",
       "      <td>2.44371</td>\n",
       "      <td>8.98841</td>\n",
       "      <td>3.62596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013747</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>0.017701</td>\n",
       "      <td>-61.2916</td>\n",
       "      <td>-39.6182</td>\n",
       "      <td>-58.1499</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.47</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.25966</td>\n",
       "      <td>9.09415</td>\n",
       "      <td>3.43015</td>\n",
       "      <td>2.42877</td>\n",
       "      <td>9.01871</td>\n",
       "      <td>3.61081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007649</td>\n",
       "      <td>-0.013923</td>\n",
       "      <td>0.014498</td>\n",
       "      <td>-60.8509</td>\n",
       "      <td>-39.0821</td>\n",
       "      <td>-58.1478</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.48</td>\n",
       "      <td>0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.29745</td>\n",
       "      <td>8.90450</td>\n",
       "      <td>3.46984</td>\n",
       "      <td>2.39736</td>\n",
       "      <td>8.94335</td>\n",
       "      <td>3.53551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>0.020352</td>\n",
       "      <td>-61.5302</td>\n",
       "      <td>-38.7240</td>\n",
       "      <td>-58.3860</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  activity_id  heart_rate  imu_wrist_temp  imu_wrist_accel16_x  \\\n",
       "0       8.39            0         NaN            30.0              2.18837   \n",
       "1       8.40            0         NaN            30.0              2.37357   \n",
       "2       8.41            0         NaN            30.0              2.07473   \n",
       "3       8.42            0         NaN            30.0              2.22936   \n",
       "4       8.43            0         NaN            30.0              2.29959   \n",
       "5       8.44            0         NaN            30.0              2.33738   \n",
       "6       8.45            0         NaN            30.0              2.37142   \n",
       "7       8.46            0         NaN            30.0              2.33951   \n",
       "8       8.47            0         NaN            30.0              2.25966   \n",
       "9       8.48            0       104.0            30.0              2.29745   \n",
       "\n",
       "   imu_wrist_accel16_y  imu_wrist_accel16_z  imu_wrist_accel6_x  \\\n",
       "0              8.56560              3.66179             2.39494   \n",
       "1              8.60107              3.54898             2.30514   \n",
       "2              8.52853              3.66021             2.33528   \n",
       "3              8.83122              3.70000             2.23055   \n",
       "4              8.82929              3.54710             2.26132   \n",
       "5              8.82900              3.54767             2.27703   \n",
       "6              9.05500              3.39347             2.39786   \n",
       "7              9.13251              3.54668             2.44371   \n",
       "8              9.09415              3.43015             2.42877   \n",
       "9              8.90450              3.46984             2.39736   \n",
       "\n",
       "   imu_wrist_accel6_y  imu_wrist_accel6_z  ...  imu_ankle_gyro_x  \\\n",
       "0             8.55081             3.64207  ...         -0.006577   \n",
       "1             8.53644             3.73280  ...          0.003014   \n",
       "2             8.53622             3.73277  ...          0.003175   \n",
       "3             8.59741             3.76295  ...          0.012698   \n",
       "4             8.65762             3.77788  ...         -0.006089   \n",
       "5             8.77828             3.73230  ...         -0.031973   \n",
       "6             8.89814             3.64131  ...         -0.019643   \n",
       "7             8.98841             3.62596  ...          0.013747   \n",
       "8             9.01871             3.61081  ...          0.007649   \n",
       "9             8.94335             3.53551  ...          0.078900   \n",
       "\n",
       "   imu_ankle_gyro_y  imu_ankle_gyro_z  imu_ankle_magnet_x  imu_ankle_magnet_y  \\\n",
       "0         -0.004638          0.000368            -59.8479            -38.8919   \n",
       "1          0.000148          0.022495            -60.7361            -39.4138   \n",
       "2         -0.020301          0.011275            -60.4091            -38.7635   \n",
       "3         -0.014303         -0.002823            -61.5199            -39.3879   \n",
       "4         -0.016024          0.001050            -60.2954            -38.8778   \n",
       "5         -0.053934          0.015594            -60.6307            -38.8676   \n",
       "6         -0.039937         -0.000785            -60.5171            -38.9819   \n",
       "7         -0.010042          0.017701            -61.2916            -39.6182   \n",
       "8         -0.013923          0.014498            -60.8509            -39.0821   \n",
       "9          0.002283          0.020352            -61.5302            -38.7240   \n",
       "\n",
       "   imu_ankle_magnet_z  imu_ankle_orient1  imu_ankle_orient2  \\\n",
       "0            -58.5253                1.0                0.0   \n",
       "1            -58.3999                1.0                0.0   \n",
       "2            -58.3956                1.0                0.0   \n",
       "3            -58.2694                1.0                0.0   \n",
       "4            -58.3977                1.0                0.0   \n",
       "5            -58.2711                1.0                0.0   \n",
       "6            -58.2733                1.0                0.0   \n",
       "7            -58.1499                1.0                0.0   \n",
       "8            -58.1478                1.0                0.0   \n",
       "9            -58.3860                1.0                0.0   \n",
       "\n",
       "   imu_ankle_orient3  imu_ankle_orient4  \n",
       "0                0.0                0.0  \n",
       "1                0.0                0.0  \n",
       "2                0.0                0.0  \n",
       "3                0.0                0.0  \n",
       "4                0.0                0.0  \n",
       "5                0.0                0.0  \n",
       "6                0.0                0.0  \n",
       "7                0.0                0.0  \n",
       "8                0.0                0.0  \n",
       "9                0.0                0.0  \n",
       "\n",
       "[10 rows x 54 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are the raw labels, activity_ids, coming in from the dataset\n",
    "\n",
    "List of activityIDs and corresponding activities:\n",
    "0=other (transient activities),\n",
    "1=lying,\n",
    "2=sitting,\n",
    "3=standing,\n",
    "4=walking,\n",
    "5=running,\n",
    "6=cycling,\n",
    "7=Nordic walking,\n",
    "9=watching TV,\n",
    "10=computer work,\n",
    "11=car driving,\n",
    "12=ascending stairs,\n",
    "13=descending stairs,\n",
    "16=vacuum cleaning,\n",
    "17=ironing,\n",
    "18=folding laundry,\n",
    "19=house cleaning,\n",
    "20=playing soccer,\n",
    "24=rope jumping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows BEFORE drop: 20326464\n",
      "Number of rows AFTER drop: 1840806\n"
     ]
    }
   ],
   "source": [
    "# Note: unless we impute the heart_rate data, we will lose most of our dataset to 'NaN' dropping\n",
    "print('Number of rows BEFORE drop: {}'.format(df.size))\n",
    "df.dropna(inplace=True)\n",
    "# df.drop_duplicates(inplace=True)\n",
    "print('Number of rows AFTER drop: {}'.format(df.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can replace the raw activity_id (integers) with their label meanings\n",
    "\n",
    "raw_activity_ids = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
    "\n",
    "activity_labels = ['unassigned', 'lying', 'sitting', 'standing ', 'walking', 'running', 'cycling', \n",
    "                'nordic_walking ', 'missing_8', 'watching_tv ', 'computer_work ', 'car_driving ', \n",
    "                'ascending_stairs ', 'descending_stairs ', 'missing_14', 'missing_15', \n",
    "                'vacuuming', 'ironing', 'folding_laundry', 'house_cleaning', 'playing soccer',\n",
    "                'missing_21', 'missing_22', 'missing_23', 'rope jumping'] \n",
    "\n",
    "df.replace(raw_activity_ids, activity_labels, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition to activity label[0]: unassigned\n",
      "Transition to activity label[2927]: lying\n",
      "Transition to activity label[30114]: sitting\n",
      "Transition to activity label[53594]: standing \n",
      "Transition to activity label[75311]: unassigned\n",
      "Transition to activity label[84966]: ironing\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at 'activity_id' label and show each transition in the first 100K rows\n",
    "prev_activity = None\n",
    "for idx in range(0,100000,1):\n",
    "    row = df.iloc[idx]\n",
    "    #print('row[{}]: {}'.format(idx, row))\n",
    "    if (row['activity_id'] != prev_activity):\n",
    "        print('Transition to new activity label[{}]: {}'.format(idx, row['activity_id']))\n",
    "        prev_activity = row['activity_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp              34089\n",
       "activity_id               13\n",
       "heart_rate               106\n",
       "imu_wrist_temp            63\n",
       "imu_wrist_accel16_x    32425\n",
       "imu_wrist_accel16_y    32527\n",
       "imu_wrist_accel16_z    32492\n",
       "imu_wrist_accel6_x     32315\n",
       "imu_wrist_accel6_y     32919\n",
       "imu_wrist_accel6_z     32368\n",
       "imu_wrist_gyro_x       33910\n",
       "imu_wrist_gyro_y       33883\n",
       "imu_wrist_gyro_z       33860\n",
       "imu_wrist_magnet_x     33464\n",
       "imu_wrist_magnet_y     32978\n",
       "imu_wrist_magnet_z     33370\n",
       "imu_wrist_orient1          1\n",
       "imu_wrist_orient2          1\n",
       "imu_wrist_orient3          1\n",
       "imu_wrist_orient4          1\n",
       "imu_chest_temp            88\n",
       "imu_chest_accel16_x    31183\n",
       "imu_chest_accel16_y    29647\n",
       "imu_chest_accel16_z    30940\n",
       "imu_chest_accel6_x     33222\n",
       "imu_chest_accel6_y     29860\n",
       "imu_chest_accel6_z     32419\n",
       "imu_chest_gyro_x       33868\n",
       "imu_chest_gyro_y       33886\n",
       "imu_chest_gyro_z       33863\n",
       "imu_chest_magnet_x     33231\n",
       "imu_chest_magnet_y     32476\n",
       "imu_chest_magnet_z     33227\n",
       "imu_chest_orient1          1\n",
       "imu_chest_orient2          1\n",
       "imu_chest_orient3          1\n",
       "imu_chest_orient4          1\n",
       "imu_ankle_temp            76\n",
       "imu_ankle_accel16_x    29936\n",
       "imu_ankle_accel16_y    31073\n",
       "imu_ankle_accel16_z    30899\n",
       "imu_ankle_accel6_x     29682\n",
       "imu_ankle_accel6_y     31050\n",
       "imu_ankle_accel6_z     32249\n",
       "imu_ankle_gyro_x       33785\n",
       "imu_ankle_gyro_y       33866\n",
       "imu_ankle_gyro_z       33885\n",
       "imu_ankle_magnet_x     32845\n",
       "imu_ankle_magnet_y     33256\n",
       "imu_ankle_magnet_z     33152\n",
       "imu_ankle_orient1          1\n",
       "imu_ankle_orient2          1\n",
       "imu_ankle_orient3          1\n",
       "imu_ankle_orient4          1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's find out how many unique/discrete values per column in our dataset\n",
    "df.nunique(axis='index') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ascending_stairs ' 'cycling' 'descending_stairs ' 'ironing' 'lying'\n",
      " 'nordic_walking ' 'rope jumping' 'running' 'sitting' 'standing '\n",
      " 'unassigned' 'vacuuming' 'walking']\n"
     ]
    }
   ],
   "source": [
    "# Let's get the distinct list of unique activities\n",
    "print(np.unique(df['activity_id'].to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <Modified down to here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook cell writes a file `preprocessing.py`, which contains the pre-processing script. You can update the script, and rerun this cell to overwrite `preprocessing.py`. You run this as a processing job in the next cell. In this script, you\n",
    "\n",
    "* Remove duplicates and rows with conflicting data\n",
    "* transform the target `income` column into a column containing two labels.\n",
    "* transform the `age` and `num persons worked for employer` numerical columns into categorical features by binning them\n",
    "* scale the continuous `capital gains`, `capital losses`, and `dividends from stocks` so they're suitable for training\n",
    "* encode the `education`, `major industry code`, `class of worker` so they're suitable for training\n",
    "* split the data into training and test datasets, and saves the training features and labels and test features and labels.\n",
    "\n",
    "Our training script will use the pre-processed training features and labels to train a model, and our model evaluation script will use the trained model and pre-processed test features and labels to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelBinarizer, KBinsDiscretizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "column_names = ['timestamp', 'activity_id', 'heart_rate', 'imu_wrist_temp', \n",
    "  'imu_wrist_accel16_x', 'imu_wrist_accel16_y', 'imu_wrist_accel16_z', \n",
    "  'imu_wrist_accel6_x', 'imu_wrist_accel6_y', 'imu_wrist_accel6_z', \n",
    "  'imu_wrist_gyro_x', 'imu_wrist_gyro_y', 'imu_wrist_gyro_z', \n",
    "  'imu_wrist_magnet_x', 'imu_wrist_magnet_y', 'imu_wrist_magnet_z', \n",
    "  'imu_wrist_orient1', 'imu_wrist_orient2', 'imu_wrist_orient3', 'imu_wrist_orient4', \n",
    "  'imu_chest_temp', 'imu_chest_accel16_x', 'imu_chest_accel16_y', 'imu_chest_accel16_z', \n",
    "  'imu_chest_accel6_x', 'imu_chest_accel6_y', 'imu_chest_accel6_z', \n",
    "  'imu_chest_gyro_x', 'imu_chest_gyro_y', 'imu_chest_gyro_z', \n",
    "  'imu_chest_magnet_x', 'imu_chest_magnet_y', 'imu_chest_magnet_z', \n",
    "  'imu_chest_orient1', 'imu_chest_orient2', 'imu_chest_orient3', 'imu_chest_orient4', \n",
    "  'imu_ankle_temp', 'imu_ankle_accel16_x', 'imu_ankle_accel16_y', 'imu_ankle_accel16_z', \n",
    "  'imu_ankle_accel6_x', 'imu_ankle_accel6_y', 'imu_ankle_accel6_z', \n",
    "  'imu_ankle_gyro_x', 'imu_ankle_gyro_y', 'imu_ankle_gyro_z', \n",
    "  'imu_ankle_magnet_x', 'imu_ankle_magnet_y', 'imu_ankle_magnet_z', \n",
    "  'imu_ankle_orient1', 'imu_ankle_orient2', 'imu_ankle_orient3', 'imu_ankle_orient4']\n",
    "\n",
    "\n",
    "raw_activity_ids = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
    "\n",
    "activity_labels = ['unassigned', 'lying', 'sitting', 'standing ', 'walking', 'running', 'cycling', \n",
    "                'nordic_walking ', 'missing_8', 'watching_tv ', 'computer_work ', 'car_driving ', \n",
    "                'ascending_stairs ', 'descending_stairs ', 'missing_14', 'missing_15', \n",
    "                'vacuuming', 'ironing', 'folding_laundry', 'house_cleaning', 'playing soccer',\n",
    "                'missing_21', 'missing_22', 'missing_23', 'rope jumping'] \n",
    "\n",
    "\n",
    "def print_shape(df):\n",
    "    negative_examples, positive_examples = np.bincount(df['income'])\n",
    "    print('Data shape: {}, {} positive examples, {} negative examples'.format(df.shape, positive_examples, negative_examples))\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train-test-split-ratio', type=float, default=0.3)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print('Received arguments {}'.format(args))\n",
    "\n",
    "    input_data_path = os.path.join('/opt/ml/processing/input', 'subject101.csv')\n",
    "    \n",
    "    print('Reading input data from {}'.format(input_data_path))\n",
    "    df_raw = pd.read_csv(input_data_path)\n",
    "    df = pd.DataFrame(data=df_raw, columns=column_names)\n",
    "    \n",
    "    print('Number of rows BEFORE drop: {}'.format(df.size))\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print('Number of rows AFTER drop: {}'.format(df.size))\n",
    "    \n",
    "    df.replace(raw_activity_ids, activity_labels, inplace=True)\n",
    "    \n",
    "    negative_examples, positive_examples = np.bincount(df['income'])\n",
    "    print('Data after cleaning: {}, {} positive examples, {} negative examples'.format(df.shape, positive_examples, negative_examples))\n",
    "    \n",
    "    split_ratio = args.train_test_split_ratio\n",
    "    print('Splitting data into train and test sets with ratio {}'.format(split_ratio))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop('income', axis=1), df['income'], test_size=split_ratio, random_state=0)\n",
    "\n",
    "    preprocess = make_column_transformer(\n",
    "        (['age', 'num persons worked for employer'], KBinsDiscretizer(encode='onehot-dense', n_bins=10)),\n",
    "        (['capital gains', 'capital losses', 'dividends from stocks'], StandardScaler()),\n",
    "        (['education', 'major industry code', 'class of worker'], OneHotEncoder(sparse=False))\n",
    "    )\n",
    "    print('Running preprocessing and feature engineering transformations')\n",
    "    train_features = preprocess.fit_transform(X_train)\n",
    "    test_features = preprocess.transform(X_test)\n",
    "    \n",
    "    print('Train data shape after preprocessing: {}'.format(train_features.shape))\n",
    "    print('Test data shape after preprocessing: {}'.format(test_features.shape))\n",
    "    \n",
    "    train_features_output_path = os.path.join('/opt/ml/processing/train', 'train_features.csv')\n",
    "    train_labels_output_path = os.path.join('/opt/ml/processing/train', 'train_labels.csv')\n",
    "    \n",
    "    test_features_output_path = os.path.join('/opt/ml/processing/test', 'test_features.csv')\n",
    "    test_labels_output_path = os.path.join('/opt/ml/processing/test', 'test_labels.csv')\n",
    "    \n",
    "    print('Saving training features to {}'.format(train_features_output_path))\n",
    "    pd.DataFrame(train_features).to_csv(train_features_output_path, header=False, index=False)\n",
    "    \n",
    "    print('Saving test features to {}'.format(test_features_output_path))\n",
    "    pd.DataFrame(test_features).to_csv(test_features_output_path, header=False, index=False)\n",
    "    \n",
    "    print('Saving training labels to {}'.format(train_labels_output_path))\n",
    "    y_train.to_csv(train_labels_output_path, header=False, index=False)\n",
    "    \n",
    "    print('Saving test labels to {}'.format(test_labels_output_path))\n",
    "    y_test.to_csv(test_labels_output_path, header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this script as a processing job. Use the `SKLearnProcessor.run()` method. You give the `run()` method one `ProcessingInput` where the `source` is the census dataset in Amazon S3, and the `destination` is where the script reads this data from, in this case `/opt/ml/processing/input`. These local paths inside the processing container must begin with `/opt/ml/processing/`.\n",
    "\n",
    "Also give the `run()` method a `ProcessingOutput`, where the `source` is the path the script writes output data to. For outputs, the `destination` defaults to an S3 bucket that the Amazon SageMaker Python SDK creates for you, following the format `s3://sagemaker-<region>-<account_id>/<processing_job_name>/output/<output_name/`. You also give the ProcessingOutputs values for `output_name`, to make it easier to retrieve these output artifacts after the job is run.\n",
    "\n",
    "The `arguments` parameter in the `run()` method are command-line arguments in our `preprocessing.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "sklearn_processor.run(code='preprocessing.py',\n",
    "                      inputs=[ProcessingInput(\n",
    "                        source=input_data_uri,\n",
    "                        destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(output_name='train_data',\n",
    "                                                source='/opt/ml/processing/train'),\n",
    "                               ProcessingOutput(output_name='test_data',\n",
    "                                                source='/opt/ml/processing/test')],\n",
    "                      arguments=['--train-test-split-ratio', '0.2']\n",
    "                     )\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()\n",
    "\n",
    "output_config = preprocessing_job_description['ProcessingOutputConfig']\n",
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'train_data':\n",
    "        preprocessed_training_data = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'test_data':\n",
    "        preprocessed_test_data = output['S3Output']['S3Uri']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now inspect the output of the pre-processing job, which consists of the processed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = pd.read_csv(preprocessed_training_data + '/train_features.csv', nrows=10)\n",
    "print('Training features shape: {}'.format(training_features.shape))\n",
    "training_features.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using the pre-processed data\n",
    "\n",
    "We create a `SKLearn` instance, which we will use to run a training job using the training script `train.py`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "sklearn = SKLearn(\n",
    "    entry_point='train.py',\n",
    "    train_instance_type=\"ml.m5.xlarge\",\n",
    "    role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training script `train.py` trains a logistic regression model on the training data, and saves the model to the `/opt/ml/model` directory, which Amazon SageMaker tars and uploads into a `model.tar.gz` file into S3 at the end of the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    training_data_directory = '/opt/ml/input/data/train'\n",
    "    train_features_data = os.path.join(training_data_directory, 'train_features.csv')\n",
    "    train_labels_data = os.path.join(training_data_directory, 'train_labels.csv')\n",
    "    print('Reading input data')\n",
    "    X_train = pd.read_csv(train_features_data, header=None)\n",
    "    y_train = pd.read_csv(train_labels_data, header=None)\n",
    "\n",
    "    model = LogisticRegression(class_weight='balanced', solver='lbfgs')\n",
    "    print('Training LR model')\n",
    "    model.fit(X_train, y_train)\n",
    "    model_output_directory = os.path.join('/opt/ml/model', \"model.joblib\")\n",
    "    print('Saving model to {}'.format(model_output_directory))\n",
    "    joblib.dump(model, model_output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training job using `train.py` on the preprocessed training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.fit({'train': preprocessed_training_data})\n",
    "training_job_description = sklearn.jobs[-1].describe()\n",
    "model_data_s3_uri = '{}{}/{}'.format(\n",
    "    training_job_description['OutputDataConfig']['S3OutputPath'],\n",
    "    training_job_description['TrainingJobName'],\n",
    "    'output/model.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "`evaluation.py` is the model evaluation script. Since the script also runs using scikit-learn as a dependency,  run this using the `SKLearnProcessor` you created previously. This script takes the trained model and the test dataset as input, and produces a JSON file containing classification evaluation metrics, including precision, recall, and F1 score for each label, and accuracy and ROC AUC for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile evaluation.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    model_path = os.path.join('/opt/ml/processing/model', 'model.tar.gz')\n",
    "    print('Extracting model from path: {}'.format(model_path))\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path='.')\n",
    "    print('Loading model')\n",
    "    model = joblib.load('model.joblib')\n",
    "\n",
    "    print('Loading test input data')\n",
    "    test_features_data = os.path.join('/opt/ml/processing/test', 'test_features.csv')\n",
    "    test_labels_data = os.path.join('/opt/ml/processing/test', 'test_labels.csv')\n",
    "\n",
    "    X_test = pd.read_csv(test_features_data, header=None)\n",
    "    y_test = pd.read_csv(test_labels_data, header=None)\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    print('Creating classification evaluation report')\n",
    "    report_dict = classification_report(y_test, predictions, output_dict=True)\n",
    "    report_dict['accuracy'] = accuracy_score(y_test, predictions)\n",
    "    report_dict['roc_auc'] = roc_auc_score(y_test, predictions)\n",
    "\n",
    "    print('Classification report:\\n{}'.format(report_dict))\n",
    "\n",
    "    evaluation_output_path = os.path.join('/opt/ml/processing/evaluation', 'evaluation.json')\n",
    "    print('Saving classification report to {}'.format(evaluation_output_path))\n",
    "\n",
    "    with open(evaluation_output_path, 'w') as f:\n",
    "        f.write(json.dumps(report_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "sklearn_processor.run(code='evaluation.py',\n",
    "                      inputs=[ProcessingInput(\n",
    "                                  source=model_data_s3_uri,\n",
    "                                  destination='/opt/ml/processing/model'),\n",
    "                              ProcessingInput(\n",
    "                                  source=preprocessed_test_data,\n",
    "                                  destination='/opt/ml/processing/test')],\n",
    "                      outputs=[ProcessingOutput(output_name='evaluation',\n",
    "                                  source='/opt/ml/processing/evaluation')]\n",
    "                     )                    \n",
    "evaluation_job_description = sklearn_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now retrieve the file `evaluation.json` from Amazon S3, which contains the evaluation report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_output_config = evaluation_job_description['ProcessingOutputConfig']\n",
    "for output in evaluation_output_config['Outputs']:\n",
    "    if output['OutputName'] == 'evaluation':\n",
    "        evaluation_s3_uri = output['S3Output']['S3Uri'] + '/evaluation.json'\n",
    "        break\n",
    "\n",
    "evaluation_output = S3Downloader.read_file(evaluation_s3_uri)\n",
    "evaluation_output_dict = json.loads(evaluation_output)\n",
    "print(json.dumps(evaluation_output_dict, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running processing jobs with your own dependencies\n",
    "\n",
    "Above, you used a processing container that has scikit-learn installed, but you can run your own processing container in your processing job as well, and still provide a script to run within your processing container.\n",
    "\n",
    "Below, you walk through how to create a processing container, and how to use a `ScriptProcessor` to run your own code within a container. Create a scikit-learn container and run a processing job using the same `preprocessing.py` script you used above. You can provide your own dependencies inside this container to run your processing script with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Dockerfile to create the processing container. Install `pandas` and `scikit-learn` into it. You can install your own dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker/Dockerfile\n",
    "\n",
    "FROM python:3.7-slim-buster\n",
    "\n",
    "RUN pip3 install pandas==0.25.3 scikit-learn==0.21.3\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "\n",
    "ENTRYPOINT [\"python3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code builds the container using the `docker` command, creates an Amazon Elastic Container Registry (Amazon ECR) repository, and pushes the image to Amazon ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "ecr_repository = 'sagemaker-processing-container'\n",
    "tag = ':latest'\n",
    "\n",
    "uri_suffix = 'amazonaws.com'\n",
    "if region in ['cn-north-1', 'cn-northwest-1']:\n",
    "    uri_suffix = 'amazonaws.com.cn'\n",
    "processing_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository + tag)\n",
    "\n",
    "# Create ECR repository and push docker image\n",
    "!docker build -t $ecr_repository docker\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $processing_repository_uri\n",
    "!docker push $processing_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ScriptProcessor` class lets you run a command inside this container, which you can use to run your own script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "script_processor = ScriptProcessor(command=['python3'],\n",
    "                image_uri=processing_repository_uri,\n",
    "                role=role,\n",
    "                instance_count=1,\n",
    "                instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the same `preprocessing.py` script you ran above, but now, this code is running inside of the Docker container you built in this notebook, not the scikit-learn image maintained by Amazon SageMaker. You can add the dependencies to the Docker image, and run your own pre-processing, feature-engineering, and model evaluation scripts inside of this container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_processor.run(code='preprocessing.py',\n",
    "                      inputs=[ProcessingInput(\n",
    "                        source=input_data_uri,\n",
    "                        destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(output_name='train_data',\n",
    "                                                source='/opt/ml/processing/train'),\n",
    "                               ProcessingOutput(output_name='test_data',\n",
    "                                                source='/opt/ml/processing/test')],\n",
    "                      arguments=['--train-test-split-ratio', '0.2']\n",
    "                     )\n",
    "script_processor_job_description = script_processor.jobs[-1].describe()\n",
    "print(script_processor_job_description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
